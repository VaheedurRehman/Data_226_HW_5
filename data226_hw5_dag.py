# -*- coding: utf-8 -*-
"""data226_hw5_dag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11pNup61X-0fJduygKhWdHGmSjHB6zuti
"""

from datetime import datetime, timedelta
import pandas as pd
import requests
import json
from airflow.decorators import dag, task
from airflow.models import Variable
from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook


# ---------------- DAG Definition ----------------
@dag(
    dag_id="data226_hw5_stock_pipeline",
    schedule_interval=None,
    start_date=datetime(2025, 10, 1),
    catchup=False,
    tags=["data226", "homework5", "airflow"],
    description="Fetch daily stock data from Alpha Vantage API and load into Snowflake with transactional full refresh."
)
def stock_pipeline():

    # ---------- 1Ô∏è‚É£ Extract Task ----------
    @task()
    def fetch_data():
        """Fetch 90 days of stock data from Alpha Vantage API"""
        symbol = "IBM"  # same as HW4
        api_key = Variable.get("ALPHA_VANTAGE_API_KEY")  # (+1) Airflow Variable
        url = f"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&apikey={api_key}&outputsize=compact"

        response = requests.get(url, timeout=60)
        data = response.json()

        if "Time Series (Daily)" not in data:
            raise ValueError("API Error or Limit Reached")

        ts = data["Time Series (Daily)"]

        rows = []
        for dt, vals in ts.items():
            rows.append({
                "symbol": symbol,
                "date": pd.to_datetime(dt).date().isoformat(),
                "open": float(vals["1. open"]),
                "high": float(vals["2. high"]),
                "low": float(vals["3. low"]),
                "close": float(vals["4. close"]),
                "volume": int(vals["5. volume"])
            })

        df = pd.DataFrame(rows).sort_values("date")
        print(f"‚úÖ Extracted {len(df)} records for {symbol}")
        return df.to_json(orient="records", date_format="iso")

    # ---------- 2Ô∏è‚É£ Transform Task ----------
    @task()
    def transform_data(json_data: str):
        """Filter last 90 days and ensure numeric types"""
        df = pd.DataFrame(json.loads(json_data))
        df["date"] = pd.to_datetime(df["date"])
        df = df[df["date"] >= (pd.Timestamp.today() - pd.Timedelta(days=90))]
        for col in ["open", "high", "low", "close"]:
            df[col] = pd.to_numeric(df[col], errors="coerce")
        df["volume"] = pd.to_numeric(df["volume"], errors="coerce").fillna(0).astype(int)
        print(f"‚úÖ Transformed dataset to {len(df)} records (last 90 days).")
        return df.to_json(orient="records", date_format="iso")

    # ---------- 3Ô∏è‚É£ Load Task ----------
    @task()
    def load_to_snowflake(json_data: str):
        """Full refresh into Snowflake table using transaction"""
        df = pd.DataFrame(json.loads(json_data))
        hook = SnowflakeHook(snowflake_conn_id="snowflake_conn")  # (+2) Connection usage
        conn = hook.get_conn()
        cursor = conn.cursor()

        print("üîó Connected to Snowflake, beginning transaction...")

        # Ensure schema + table
        ddl_schema = "CREATE SCHEMA IF NOT EXISTS RAW;"
        ddl_table = """
        CREATE TABLE IF NOT EXISTS RAW.DAILY_PRICES (
            SYMBOL STRING NOT NULL,
            DATE DATE NOT NULL,
            OPEN FLOAT,
            HIGH FLOAT,
            LOW FLOAT,
            CLOSE FLOAT,
            VOLUME NUMBER,
            CONSTRAINT PK_SYMBOL_DATE PRIMARY KEY (SYMBOL, DATE)
        );
        """
        cursor.execute(ddl_schema)
        cursor.execute(ddl_table)

        cursor.execute("BEGIN;")
        try:
            # Full refresh (delete + insert)
            cursor.execute("DELETE FROM RAW.DAILY_PRICES;")
            insert_sql = """
                INSERT INTO RAW.DAILY_PRICES (SYMBOL, DATE, OPEN, HIGH, LOW, CLOSE, VOLUME)
                VALUES (%(symbol)s, %(date)s, %(open)s, %(high)s, %(low)s, %(close)s, %(volume)s)
            """
            records = df.to_dict("records")
            cursor.executemany(insert_sql, records)
            cursor.execute("COMMIT;")
            print(f"‚úÖ Transaction complete ‚Äî {len(records)} rows loaded.")
        except Exception as e:
            cursor.execute("ROLLBACK;")
            print("‚ùå Error occurred, rolled back transaction:", e)
            raise
        finally:
            cursor.close()
            conn.close()
            print("üîí Snowflake connection closed.")

    # ---------- 4Ô∏è‚É£ Task Dependency Chain ----------
    raw_data = fetch_data()
    transformed = transform_data(raw_data)
    load_to_snowflake(transformed)


# Register DAG
dag = stock_pipeline()