# DATA 226 â€“ Homework 5: Airflow DAG

**Author:** Vaheedur Rehman Mahmed  
**Course:** DATA 226 â€“ Data Engineering  
**Instructor:** Prof. Vishnu Pendyala  
**University:** San JosÃ© State University  

---

## ğŸ§  Overview
This assignment extends Homework 4 by porting the ETL pipeline into **Apache Airflow**.  
The DAG fetches 90 days of stock price data from the **Alpha Vantage API**, performs light transformations using Pandas, and loads the data into **Snowflake** using a full-refresh transaction.

---

## âš™ï¸ Files in This Repository
| File | Description |
|------|--------------|
| `data226_hw5_dag.py` | Airflow DAG file implementing the ETL pipeline |
| `docker-compose.yaml` | Airflow environment setup for Docker Compose |

---

## ğŸ§© DAG Structure
**DAG ID:** `data226_hw5_stock_pipeline`

**Tasks:**
1. `fetch_data` â†’ Fetches stock data from Alpha Vantage API  
2. `transform_data` â†’ Cleans and filters the last 90 days of data  
3. `load_to_snowflake` â†’ Loads transformed data into Snowflake via full-refresh SQL transaction

**Task Flow:**
fetch_data â†’ transform_data â†’ load_to_snowflake

---

## ğŸ”‘ Airflow Setup Steps
1. **Add Variable**
   - Go to: *Admin â†’ Variables â†’ + Add*
   - Key: `ALPHA_VANTAGE_API_KEY`
   - Value: `<your Alpha Vantage API key>`

2. **Add Connection**
   - Go to: *Admin â†’ Connections â†’ + Add*
   - Conn ID: `snowflake_conn`
   - Conn Type: `Snowflake`
   - Fill in your Snowflake credentials (Account, User, Password, Warehouse, Database, Schema)

3. **Place DAG**
   - Copy `data226_hw5_dag.py` into your Airflow `/dags/` folder

4. **Run Airflow**
   ```bash
   docker compose up -d
